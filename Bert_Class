import pandas as pd
import re
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from transformers import BertTokenizer, TFBertModel
from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Adım 1: Veri setini indiren fonksiyon
def download_dataset():
    file_path = '/content/drive/MyDrive/bbc_data_1_2.xlsx'
    data = pd.read_excel('bbc_data_1_2.xlsx')
    return data

# Adım 2: Veriyi temizleyen fonksiyon
def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)  # Noktalama işaretlerini kaldırma
    text = re.sub(r'\s+', ' ', text)    # Birden fazla boşluğu tek boşlukla değiştirme
    text = text.lower()                 # Metni küçük harfe dönüştürme
    return text

# Adım 3: Eksik değerleri doldurma
def fill_missing_values(data):
    data.dropna(inplace=True)
    return data

# Adım 4: Sadece BERT modeli oluşturma
def create_bert_model():
    bert_model = TFBertModel.from_pretrained("bert-base-uncased")
    bert_input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')
    bert_attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')
    bert_output = bert_model(bert_input_ids, attention_mask=bert_attention_mask).last_hidden_state
    bert_pooling = GlobalMaxPooling1D()(bert_output)

    dense_output = Dense(64, activation='relu')(bert_pooling)
    final_output = Dense(5, activation='softmax')(dense_output)  # 5 sınıf örneği
    model = Model(inputs=[bert_input_ids, bert_attention_mask], outputs=final_output)

    return model

# Adım 5: Veriyi hazırlayan fonksiyon
def prepare_dataset(data, tokenizer):
    inputs = tokenizer(data['text'].tolist(), padding="max_length", truncation=True, max_length=128, return_tensors="tf")
    labels = tf.convert_to_tensor(data['category'].values)  # Kategorileri tensöre dönüştür
    return inputs, labels

# Adım 6: Modeli eğiten fonksiyon
def train_bert_model(model, inputs, labels):
    optimizer = Adam(learning_rate=2e-5)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(
        [inputs['input_ids'], inputs['attention_mask']],
        labels,
        validation_split=0.2,
        epochs=3,
        batch_size=16
    )
    return model

# Adım 7: Sonuçları raporlayan fonksiyon
def report_results(model, inputs, labels):
    predictions = model.predict([inputs['input_ids'], inputs['attention_mask']])
    preds = predictions.argmax(axis=-1)
    report = classification_report(labels, preds)
    print(report)

# Ana fonksiyon
def main():
    # Adım 1: Veri setini indirme
    data = download_dataset()

    # Adım 2: Veriyi temizleme
    data['text'] = data['text'].apply(clean_text)

    # Adım 3: Eksik değerleri doldurma
    data = fill_missing_values(data)

    # Adım 4: Tokenizer oluşturma
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    # Adım 5: Veriyi hazırlama
    inputs, labels = prepare_dataset(data, tokenizer)

    # Adım 6: BERT modeli oluşturma
    bert_model = create_bert_model()

    # Adım 7: Modeli eğitme
    trained_model = train_bert_model(bert_model, inputs, labels)

    # Adım 8: Sonuçları raporlama
    report_results(trained_model, inputs, labels)

# Ana fonksiyonu çağırma
if __name__ == "__main__":
    main()
