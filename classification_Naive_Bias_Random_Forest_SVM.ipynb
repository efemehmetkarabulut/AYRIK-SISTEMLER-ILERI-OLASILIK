{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efemehmetkarabulut/AYRIK-SISTEMLER-ILERI-OLASILIK/blob/main/classification_Naive_Bias_Random_Forest_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytYBFctLt5CI",
        "outputId": "7914d517-846f-4d76-ea37-624e0f814982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: Naive Bayes\n",
            "                                            precision    recall  f1-score   support\n",
            "\n",
            "                  Astronomy & Astrophysics       0.84      0.91      0.87      8581\n",
            "                                  Biology        0.60      0.44      0.51      7154\n",
            "                                Chemistry        0.86      0.74      0.80      8241\n",
            "Computer Science, Artificial Intelligence        0.68      0.90      0.77      8662\n",
            "              Cybersecurity & Cryptography       0.46      0.50      0.48      8601\n",
            "                      Economics & Business       0.88      0.86      0.87      7400\n",
            "                     Environmental Science       0.63      0.61      0.62      8431\n",
            "                                 Geography       0.48      0.89      0.62      9290\n",
            "                                   History       0.76      0.26      0.39      7724\n",
            "                   International Relations       0.71      0.27      0.39      8506\n",
            "       Materials Science, Coatings & Films       0.69      0.96      0.80      8428\n",
            "         Medicine, Research & Experimental       0.71      0.56      0.63      7126\n",
            "                            Neurosciences        0.70      0.77      0.73      7302\n",
            "                                     music       0.89      0.95      0.92      7435\n",
            "                                   physics       0.90      0.69      0.78      7776\n",
            "\n",
            "                                  accuracy                           0.69    120657\n",
            "                                 macro avg       0.72      0.69      0.68    120657\n",
            "                              weighted avg       0.71      0.69      0.68    120657\n",
            "\n",
            "\n",
            "Model: Random Forest\n",
            "                                            precision    recall  f1-score   support\n",
            "\n",
            "                  Astronomy & Astrophysics       0.71      0.89      0.79      8581\n",
            "                                  Biology        0.55      0.45      0.50      7154\n",
            "                                Chemistry        0.63      0.85      0.72      8241\n",
            "Computer Science, Artificial Intelligence        0.63      0.89      0.74      8662\n",
            "              Cybersecurity & Cryptography       0.61      0.50      0.55      8601\n",
            "                      Economics & Business       0.70      0.90      0.79      7400\n",
            "                     Environmental Science       0.61      0.54      0.57      8431\n",
            "                                 Geography       0.69      0.66      0.67      9290\n",
            "                                   History       0.75      0.48      0.58      7724\n",
            "                   International Relations       0.70      0.50      0.58      8506\n",
            "       Materials Science, Coatings & Films       0.81      0.86      0.84      8428\n",
            "         Medicine, Research & Experimental       0.70      0.54      0.61      7126\n",
            "                            Neurosciences        0.67      0.68      0.68      7302\n",
            "                                     music       0.95      0.95      0.95      7435\n",
            "                                   physics       0.79      0.79      0.79      7776\n",
            "\n",
            "                                  accuracy                           0.70    120657\n",
            "                                 macro avg       0.70      0.70      0.69    120657\n",
            "                              weighted avg       0.70      0.70      0.69    120657\n",
            "\n",
            "\n",
            "Model: SVM\n",
            "                                            precision    recall  f1-score   support\n",
            "\n",
            "                  Astronomy & Astrophysics       0.89      0.90      0.90      8581\n",
            "                                  Biology        0.61      0.62      0.61      7154\n",
            "                                Chemistry        0.82      0.86      0.84      8241\n",
            "Computer Science, Artificial Intelligence        0.77      0.91      0.83      8662\n",
            "              Cybersecurity & Cryptography       0.63      0.60      0.62      8601\n",
            "                      Economics & Business       0.85      0.91      0.88      7400\n",
            "                     Environmental Science       0.68      0.69      0.69      8431\n",
            "                                 Geography       0.74      0.81      0.77      9290\n",
            "                                   History       0.76      0.63      0.69      7724\n",
            "                   International Relations       0.76      0.59      0.67      8506\n",
            "       Materials Science, Coatings & Films       0.87      0.93      0.90      8428\n",
            "         Medicine, Research & Experimental       0.75      0.70      0.72      7126\n",
            "                            Neurosciences        0.76      0.80      0.78      7302\n",
            "                                     music       0.99      0.96      0.97      7435\n",
            "                                   physics       0.88      0.83      0.85      7776\n",
            "\n",
            "                                  accuracy                           0.78    120657\n",
            "                                 macro avg       0.78      0.78      0.78    120657\n",
            "                              weighted avg       0.78      0.78      0.78    120657\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def download_dataset():\n",
        "    file_path = '/content/drive/MyDrive/Text Classification Tez Çalışması/inputs for final task/bbc_data_Format_151K.xlsx'\n",
        "    try:\n",
        "        data = pd.read_excel(file_path)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Veri seti yüklenirken hata oluştu: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def fill_missing_values(data):\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "def split_data(data):\n",
        "    X = data['text']\n",
        "    y = data['category']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
        "    classifiers = {\n",
        "        \"Naive Bayes\": MultinomialNB(),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"SVM\": SVC(kernel='linear', random_state=42)\n",
        "    }\n",
        "\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"\\nModel: {name}\")\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_df=0.75, ngram_range=(1, 2))),\n",
        "            ('clf', clf)\n",
        "        ])\n",
        "\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        predictions = pipeline.predict(X_test)\n",
        "        print(classification_report(y_test, predictions))\n",
        "\n",
        "def main():\n",
        "    data = download_dataset()\n",
        "    if data is None:\n",
        "        return\n",
        "\n",
        "    data['text'] = data['text'].apply(clean_text)\n",
        "    data = fill_missing_values(data)\n",
        "    X_train, X_test, y_train, y_test = split_data(data)\n",
        "    train_and_evaluate(X_train, X_test, y_train, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oteTyWX5HwOd",
        "outputId": "88f16561-424b-4e89-b459-2fc9f1b7c24b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: Naive Bayes\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                                            precision    recall  f1-score   support\n",
            "\n",
            "                  Astronomy & Astrophysics       0.87      0.93      0.90      8550\n",
            "                                  Biology        0.67      0.43      0.53      7199\n",
            "                                Chemistry        0.90      0.74      0.81      8177\n",
            "Computer Science, Artificial Intelligence        0.76      0.93      0.84      8623\n",
            "                      Economics & Business       0.95      0.85      0.90      7440\n",
            "                     Environmental Science       0.84      0.70      0.76      8476\n",
            "                                 Geography       0.63      0.92      0.75      9347\n",
            "       Materials Science, Coatings & Films       0.74      0.98      0.84      8285\n",
            "         Medicine, Research & Experimental       0.80      0.61      0.69      7121\n",
            "                            Neurosciences        0.74      0.79      0.77      7327\n",
            "                                     music       0.94      0.94      0.94      7500\n",
            "                                   physics       0.95      0.69      0.80      7779\n",
            "\n",
            "                                  accuracy                           0.80     95824\n",
            "                                 macro avg       0.82      0.79      0.79     95824\n",
            "                              weighted avg       0.81      0.80      0.79     95824\n",
            "\n",
            "\n",
            "Model: Random Forest\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                                            precision    recall  f1-score   support\n",
            "\n",
            "                  Astronomy & Astrophysics       0.76      0.90      0.83      8550\n",
            "                                  Biology        0.60      0.45      0.51      7199\n",
            "                                Chemistry        0.70      0.86      0.78      8177\n",
            "Computer Science, Artificial Intelligence        0.68      0.93      0.78      8623\n",
            "                      Economics & Business       0.78      0.90      0.84      7440\n",
            "                     Environmental Science       0.82      0.66      0.73      8476\n",
            "                                 Geography       0.83      0.71      0.77      9347\n",
            "       Materials Science, Coatings & Films       0.87      0.88      0.88      8285\n",
            "         Medicine, Research & Experimental       0.74      0.58      0.65      7121\n",
            "                            Neurosciences        0.75      0.68      0.71      7327\n",
            "                                     music       0.95      0.91      0.93      7500\n",
            "                                   physics       0.84      0.80      0.82      7779\n",
            "\n",
            "                                  accuracy                           0.78     95824\n",
            "                                 macro avg       0.78      0.77      0.77     95824\n",
            "                              weighted avg       0.78      0.78      0.77     95824\n",
            "\n",
            "\n",
            "Model: SVM\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4de6594cce71>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-4de6594cce71>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_missing_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-4de6594cce71>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     48\u001b[0m         ])\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mall_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 )\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlast_step_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibsvm_sparse_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy, maxprint)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \"\"\"\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxprint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def download_dataset():\n",
        "    file_path = '/content/drive/MyDrive/Text Classification Tez Çalışması/inputs for final task/bbc_data_Format_151K.xlsx'\n",
        "    try:\n",
        "        data = pd.read_excel(file_path)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Veri seti yüklenirken hata oluştu: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.lower()\n",
        "\n",
        "def fill_missing_values(data):\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "def split_data(data):\n",
        "    X = data['text']\n",
        "    y = data['category']\n",
        "    return train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
        "    classifiers = {\n",
        "        \"Naive Bayes\": MultinomialNB(),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"SVM\": SVC(kernel='linear', random_state=42)\n",
        "    }\n",
        "\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"\\nModel: {name}\")\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_df=0.75, ngram_range=(1, 2))),\n",
        "            ('clf', clf)\n",
        "        ])\n",
        "\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        predictions = pipeline.predict(X_test)\n",
        "\n",
        "        print(\"\\nClassification Report:\\n\")\n",
        "        print(classification_report(y_test, predictions))\n",
        "\n",
        "        # Confusion Matrix\n",
        "    cm = confusion_matrix(decoded_labels, decoded_predictions, labels=label_encoder.classes_)\n",
        "    cm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False, linewidths=0.5)\n",
        "\n",
        "    plt.title(\"Confusion Matrix - BERT\", fontsize=16)\n",
        "    plt.xlabel(\"Tahmin Edilen Etiket\", fontsize=12)\n",
        "    plt.ylabel(\"Gerçek Etiket\", fontsize=12)\n",
        "\n",
        "    # X ekseni etiketlerini düzelt\n",
        "    plt.xticks(ticks=np.arange(len(label_encoder.classes_)) + 0.5,  # merkezleme\n",
        "               labels=label_encoder.classes_,\n",
        "               rotation=45,\n",
        "               ha='right')\n",
        "\n",
        "    # Y ekseni etiketleri\n",
        "    plt.yticks(ticks=np.arange(len(label_encoder.classes_)) + 0.5,  # merkezleme\n",
        "               labels=label_encoder.classes_,\n",
        "               rotation=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    data = download_dataset()\n",
        "    if data is None:\n",
        "        return\n",
        "\n",
        "    data['text'] = data['text'].apply(clean_text)\n",
        "    data = fill_missing_values(data)\n",
        "    X_train, X_test, y_train, y_test = split_data(data)\n",
        "    train_and_evaluate(X_train, X_test, y_train, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zS_SuOaeEnRA"
      },
      "outputs": [],
      "source": [
        "pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sq2juplFEpMS"
      },
      "outputs": [],
      "source": [
        "# Word2Vec + Hem Random Forest hem de SVM ile Eğitim\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import gensim.downloader as api\n",
        "\n",
        "def download_dataset():\n",
        "    file_path = '/content/drive/MyDrive/Text Classification Tez Çalışması/inputs for final task/bbc_data_Format_151K.xlsx'\n",
        "    try:\n",
        "        data = pd.read_excel(file_path)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Veri seti yüklenirken hata oluştu: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.lower()\n",
        "\n",
        "def fill_missing_values(data):\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "def split_data(data):\n",
        "    X = data['text']\n",
        "    y = data['category']\n",
        "    return train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "def get_embedding_vector(texts, model, dim):\n",
        "    vectors = []\n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        vecs = [model[word] for word in tokens if word in model]\n",
        "        if vecs:\n",
        "            vectors.append(np.mean(vecs, axis=0))\n",
        "        else:\n",
        "            vectors.append(np.zeros(dim))\n",
        "    return np.array(vectors)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False, linewidths=0.5)\n",
        "    plt.title(f\"Confusion Matrix - {title}\", fontsize=16)\n",
        "    plt.xlabel(\"Tahmin Edilen Etiket\", fontsize=12)\n",
        "    plt.ylabel(\"Gerçek Etiket\", fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_and_evaluate_word2vec(X_train, X_test, y_train, y_test):\n",
        "    print(\"Word2Vec modeli indiriliyor...\")\n",
        "    model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "    print(\"Vektörler hazırlanıyor...\")\n",
        "    X_train_vec = get_embedding_vector(X_train, model, 300)\n",
        "    X_test_vec = get_embedding_vector(X_test, model, 300)\n",
        "\n",
        "    classifiers = {\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"SVM (Linear Kernel)\": SVC(kernel='linear', random_state=42)\n",
        "    }\n",
        "\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"\\nModel: {name}\")\n",
        "        clf.fit(X_train_vec, y_train)\n",
        "        predictions = clf.predict(X_test_vec)\n",
        "\n",
        "        print(\"\\nClassification Report:\\n\")\n",
        "        print(classification_report(y_test, predictions))\n",
        "        plot_confusion_matrix(y_test, predictions, clf.classes_ if hasattr(clf, 'classes_') else np.unique(y_test), title=name)\n",
        "\n",
        "def main():\n",
        "    data = download_dataset()\n",
        "    if data is None:\n",
        "        return\n",
        "\n",
        "    data['text'] = data['text'].apply(clean_text)\n",
        "    data = fill_missing_values(data)\n",
        "    X_train, X_test, y_train, y_test = split_data(data)\n",
        "    train_and_evaluate_word2vec(X_train, X_test, y_train, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTMgqFs9Eu0D"
      },
      "outputs": [],
      "source": [
        "#GloVe ile tüm modeller aynı anda naive svm rf\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import gensim.downloader as api\n",
        "\n",
        "def download_dataset():\n",
        "    file_path = '/content/drive/MyDrive/Text Classification Tez Çalışması/inputs for final task/bbc_data_Format_151K.xlsx'\n",
        "    try:\n",
        "        data = pd.read_excel(file_path)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Veri seti yüklenirken hata oluştu: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.lower()\n",
        "\n",
        "def fill_missing_values(data):\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "def split_data(data):\n",
        "    X = data['text']\n",
        "    y = data['category']\n",
        "    return train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "def get_embedding_vector(texts, model, dim):\n",
        "    vectors = []\n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        vecs = [model[word] for word in tokens if word in model]\n",
        "        if vecs:\n",
        "            vectors.append(np.mean(vecs, axis=0))\n",
        "        else:\n",
        "            vectors.append(np.zeros(dim))\n",
        "    return np.array(vectors)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False, linewidths=0.5)\n",
        "    plt.title(f\"Confusion Matrix - {title}\", fontsize=16)\n",
        "    plt.xlabel(\"Tahmin Edilen Etiket\", fontsize=12)\n",
        "    plt.ylabel(\"Gerçek Etiket\", fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_and_evaluate_glove(X_train, X_test, y_train, y_test):\n",
        "    print(\"GloVe modeli indiriliyor...\")\n",
        "    model = api.load(\"glove-wiki-gigaword-100\")  # 100 boyutlu, hızlı ve etkili\n",
        "\n",
        "    print(\"Vektörler hazırlanıyor...\")\n",
        "    X_train_vec = get_embedding_vector(X_train, model, 100)\n",
        "    X_test_vec = get_embedding_vector(X_test, model, 100)\n",
        "\n",
        "    classifiers = {\n",
        "        \"Naive Bayes (GaussianNB)\": GaussianNB(),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"SVM (Linear Kernel)\": SVC(kernel='linear', random_state=42)\n",
        "    }\n",
        "\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\"Model: {name}\")\n",
        "        clf.fit(X_train_vec, y_train)\n",
        "        predictions = clf.predict(X_test_vec)\n",
        "\n",
        "        print(\"\\nClassification Report:\\n\")\n",
        "        print(classification_report(y_test, predictions))\n",
        "        plot_confusion_matrix(y_test, predictions, np.unique(y_test), title=name)\n",
        "\n",
        "def main():\n",
        "    data = download_dataset()\n",
        "    if data is None:\n",
        "        return\n",
        "\n",
        "    data['text'] = data['text'].apply(clean_text)\n",
        "    data = fill_missing_values(data)\n",
        "    X_train, X_test, y_train, y_test = split_data(data)\n",
        "    train_and_evaluate_glove(X_train, X_test, y_train, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eNn4ZkzQI2YG"
      },
      "outputs": [],
      "source": [
        "#FastText (300D) + Naive Bayes + Random Forest + SVM – Tüm Çıktılar\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import gensim.downloader as api\n",
        "\n",
        "def download_dataset():\n",
        "    file_path = '/content/drive/MyDrive/Text Classification Tez Çalışması/inputs for final task/bbc_data_Format_151K.xlsx'\n",
        "    try:\n",
        "        data = pd.read_excel(file_path)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Veri seti yüklenirken hata oluştu: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.lower()\n",
        "\n",
        "def fill_missing_values(data):\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "def split_data(data):\n",
        "    X = data['text']\n",
        "    y = data['category']\n",
        "    return train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "def get_embedding_vector(texts, model, dim):\n",
        "    vectors = []\n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        vecs = [model[word] for word in tokens if word in model]\n",
        "        if vecs:\n",
        "            vectors.append(np.mean(vecs, axis=0))\n",
        "        else:\n",
        "            vectors.append(np.zeros(dim))\n",
        "    return np.array(vectors)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False, linewidths=0.5)\n",
        "    plt.title(f\"Confusion Matrix - {title}\", fontsize=16)\n",
        "    plt.xlabel(\"Tahmin Edilen Etiket\", fontsize=12)\n",
        "    plt.ylabel(\"Gerçek Etiket\", fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_and_evaluate_fasttext(X_train, X_test, y_train, y_test):\n",
        "    print(\"FastText modeli yükleniyor...\")\n",
        "    model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "    print(\"Vektörler oluşturuluyor...\")\n",
        "    X_train_vec = get_embedding_vector(X_train, model, 300)\n",
        "    X_test_vec = get_embedding_vector(X_test, model, 300)\n",
        "\n",
        "    classifiers = {\n",
        "        \"Naive Bayes (GaussianNB)\": GaussianNB(),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"SVM (Linear Kernel)\": SVC(kernel='linear', random_state=42)\n",
        "    }\n",
        "\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"\\nModel: {name}\")\n",
        "        clf.fit(X_train_vec, y_train)\n",
        "        predictions = clf.predict(X_test_vec)\n",
        "\n",
        "        print(\"\\nClassification Report:\\n\")\n",
        "        print(classification_report(y_test, predictions))\n",
        "        labels = np.unique(y_test)\n",
        "        plot_confusion_matrix(y_test, predictions, labels, title=name)\n",
        "\n",
        "def main():\n",
        "    data = download_dataset()\n",
        "    if data is None:\n",
        "        return\n",
        "\n",
        "    data['text'] = data['text'].apply(clean_text)\n",
        "    data = fill_missing_values(data)\n",
        "    X_train, X_test, y_train, y_test = split_data(data)\n",
        "    train_and_evaluate_fasttext(X_train, X_test, y_train, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T3F-xCQJ9Cr_",
        "outputId": "c21406a6-fdb8-4e97-e643-a6d1bea8cb2a"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/bbc_data_Format_151K (1).xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-93bf7de32893>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Veri setini yükleme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/bbc_data_Format_151K (1).xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Veriyi temizleme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/bbc_data_Format_151K (1).xlsx'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Adım 1: Veriyi temizleyen fonksiyon\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Noktalama işaretlerini kaldır\n",
        "    text = re.sub(r'\\s+', ' ', text)    # Çoklu boşlukları tek boşlukla değiştir\n",
        "    text = text.lower()                  # Küçük harfe çevir\n",
        "    return text\n",
        "\n",
        "# Adım 2: Veriyi bölme\n",
        "def split_data(data):\n",
        "    X = data['text']\n",
        "    y = data['category']\n",
        "    return train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Adım 3: BERT modelini oluşturma\n",
        "def create_bert_model(num_labels):\n",
        "    model = TFBertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "def lr_scheduler():\n",
        "    return tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "# Adım 4: BERT modelini eğitme\n",
        "def train_bert_model(model, train_data, val_data, epochs, learning_rate):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metrics = [\"accuracy\"]\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "    scheduler = lr_scheduler()\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=2,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        train_data,\n",
        "        validation_data=val_data,\n",
        "        epochs=epochs,\n",
        "        callbacks=[scheduler, early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Adım 5: Modeli değerlendirme ve sonuçları raporlama\n",
        "def evaluate_model(model, val_data, val_labels_encoded, label_encoder):\n",
        "    predictions = model.predict(val_data)[\"logits\"]\n",
        "    predicted_labels_encoded = np.argmax(predictions, axis=1)\n",
        "\n",
        "    decoded_labels = label_encoder.inverse_transform(val_labels_encoded)\n",
        "    decoded_predictions = label_encoder.inverse_transform(predicted_labels_encoded)\n",
        "\n",
        "    report = classification_report(decoded_labels, decoded_predictions, target_names=label_encoder.classes_)\n",
        "    print(\"Sınıflandırma Raporu (BERT):\")\n",
        "    print(report)\n",
        "\n",
        "    # Sınıf başına düşen eleman sayısını yazdırma\n",
        "    print(\"\\nSınıf Başına Düşen Eleman Sayısı (Test Seti):\")\n",
        "    unique_labels, counts = np.unique(decoded_labels, return_counts=True)\n",
        "    for label, count in zip(unique_labels, counts):\n",
        "        print(f\"{label:<40} {count}\")\n",
        "\n",
        "# Ana program\n",
        "if __name__ == \"__main__\":\n",
        "    # Veri setini yükleme\n",
        "    file_path = '/content/drive/MyDrive/bbc_data_Format_151K (1).xlsx'\n",
        "    data = pd.read_excel(file_path)\n",
        "\n",
        "    # Veriyi temizleme\n",
        "    data['text'] = data['text'].apply(clean_text)\n",
        "\n",
        "    # Kategori etiketlerini kodlama\n",
        "    label_encoder = LabelEncoder()\n",
        "    data['category_encoded'] = label_encoder.fit_transform(data['category'])\n",
        "\n",
        "    # Veriyi bölme\n",
        "    X_train, X_test, y_train_encoded, y_test_encoded, y_train_labels, y_test_labels = train_test_split(\n",
        "        data['text'], data['category_encoded'], data['category'], test_size=0.8, random_state=42\n",
        "    )\n",
        "\n",
        "    # Tokenizer ve maksimum uzunluk\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    max_length = 128  # Artırıldı\n",
        "\n",
        "    train_encodings = tokenizer(list(X_train), truncation=True, padding='max_length', max_length=max_length, return_tensors=\"tf\")\n",
        "    test_encodings = tokenizer(list(X_test), truncation=True, padding='max_length', max_length=max_length, return_tensors=\"tf\")\n",
        "\n",
        "    # TF Dataset oluşturma, batch size artırıldı\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"]},\n",
        "        y_train_encoded\n",
        "    )).batch(64)  # Artırıldı\n",
        "\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]},\n",
        "        y_test_encoded\n",
        "    )).batch(64)  # Artırıldı\n",
        "\n",
        "    # Model oluştur\n",
        "    bert_model = create_bert_model(num_labels=len(label_encoder.classes_))\n",
        "\n",
        "    # Eğitimi başlat\n",
        "    trained_model = train_bert_model(\n",
        "        model=bert_model,\n",
        "        train_data=train_dataset,\n",
        "        val_data=test_dataset,\n",
        "        epochs=5,  # Artırıldı\n",
        "        learning_rate=2e-5\n",
        "    )\n",
        "\n",
        "    # Değerlendir\n",
        "    evaluate_model(\n",
        "        trained_model,\n",
        "        {\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]},\n",
        "        y_test_encoded,\n",
        "        label_encoder\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC-JzVYknf5O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Adım 1: Veriyi temizleyen fonksiyon\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Noktalama işaretlerini kaldır\n",
        "    text = re.sub(r'\\s+', ' ', text)    # Çoklu boşlukları tek boşlukla değiştir\n",
        "    text = text.lower()                  # Küçük harfe çevir\n",
        "    return text\n",
        "\n",
        "# Adım 2: Veriyi bölme\n",
        "def split_data(data):\n",
        "    X = data['text']\n",
        "    y = data['category']\n",
        "    return train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Adım 3: BERT modelini oluşturma\n",
        "def create_bert_model(num_labels):\n",
        "    model = TFBertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "def lr_scheduler():\n",
        "    return tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "# Adım 4: BERT modelini eğitme\n",
        "def train_bert_model(model, train_data, val_data, epochs, learning_rate):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metrics = [\"accuracy\"]\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "    scheduler = lr_scheduler()\n",
        "\n",
        "    model.fit(\n",
        "        train_data,\n",
        "        validation_data=val_data,\n",
        "        epochs=epochs,\n",
        "        callbacks=[scheduler],\n",
        "        verbose=1  # Eğitim sürecini daha az ayrıntılı göstererek hızı artırabiliriz.\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Adım 5: Modeli değerlendirme ve sonuçları raporlama\n",
        "def evaluate_model(model, val_data, val_labels_encoded, label_encoder):\n",
        "    predictions = model.predict(val_data)[\"logits\"]\n",
        "    predicted_labels_encoded = np.argmax(predictions, axis=1)\n",
        "\n",
        "    decoded_labels = label_encoder.inverse_transform(val_labels_encoded)\n",
        "    decoded_predictions = label_encoder.inverse_transform(predicted_labels_encoded)\n",
        "\n",
        "    report = classification_report(decoded_labels, decoded_predictions, target_names=label_encoder.classes_)\n",
        "    print(\"Sınıflandırma Raporu (BERT):\")\n",
        "    print(report)\n",
        "\n",
        "    # Sınıf başına düşen eleman sayısını yazdırma\n",
        "    print(\"\\nSınıf Başına Düşen Eleman Sayısı (Test Seti):\")\n",
        "    unique_labels, counts = np.unique(decoded_labels, return_counts=True)\n",
        "    for label, count in zip(unique_labels, counts):\n",
        "        print(f\"{label:<40} {count}\")\n",
        "\n",
        "# Ana program\n",
        "if __name__ == \"__main__\":\n",
        "    # Veri setini yükleme (Örnekte Excel dosyası kullanılıyor)\n",
        "    file_path = '/content/drive/MyDrive/Text Classification Tez Çalışması/inputs for final task/bbc_data_Format_151K.xlsx'  # Dosya yolunu doğru ayarlayın\n",
        "    data = pd.read_excel(file_path)\n",
        "\n",
        "    # Veriyi temizleme\n",
        "    data['text'] = data['text'].apply(clean_text)\n",
        "\n",
        "    # Kategori etiketlerini kodlama\n",
        "    label_encoder = LabelEncoder()\n",
        "    data['category_encoded'] = label_encoder.fit_transform(data['category'])\n",
        "\n",
        "    # Veriyi bölme\n",
        "    X_train, X_test, y_train_encoded, y_test_encoded, y_train_labels, y_test_labels = train_test_split(\n",
        "        data['text'], data['category_encoded'], data['category'], test_size=0.8, random_state=42\n",
        "    )\n",
        "\n",
        "    # BERT Tokenizer ile veriyi tokenize etme\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    max_length = 64  # Daha kısa bir max_length denenebilir\n",
        "\n",
        "    train_encodings = tokenizer(list(X_train), truncation=True, padding='max_length', max_length=max_length, return_tensors=\"tf\")\n",
        "    test_encodings = tokenizer(list(X_test), truncation=True, padding='max_length', max_length=max_length, return_tensors=\"tf\")\n",
        "\n",
        "    # TensorFlow tensörleri\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"]},\n",
        "        y_train_encoded\n",
        "    )).batch(32)  # Batch size'ı artırarak eğitimi hızlandırabiliriz\n",
        "\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]},\n",
        "        y_test_encoded\n",
        "    )).batch(32)  # Batch size'ı artırarak değerlendirmeyi hızlandırabiliriz\n",
        "\n",
        "    # BERT modelini oluşturma\n",
        "    bert_model = create_bert_model(num_labels=len(label_encoder.classes_))\n",
        "\n",
        "    # Modeli eğitme\n",
        "    trained_model = train_bert_model(\n",
        "        model=bert_model,\n",
        "        train_data=train_dataset,\n",
        "        val_data=test_dataset,\n",
        "        epochs=3,  # Epoch sayısını azaltarak süreyi kısaltabiliriz\n",
        "        learning_rate=2e-5\n",
        "    )\n",
        "\n",
        "    # Modeli değerlendirme\n",
        "    evaluate_model(\n",
        "        trained_model,\n",
        "        {\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]},\n",
        "        y_test_encoded,\n",
        "        label_encoder\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1ozkePFMBGol7U2eQ3N-8QgJ19FgjVyB3",
      "authorship_tag": "ABX9TyP/LbG9/w+EfnDeZCId5Yuw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}